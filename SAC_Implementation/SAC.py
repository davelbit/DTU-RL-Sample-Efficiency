## Imports
from copy import deepcopy

from matplotlib import pyplot as plt
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import json

import logging

import dmc2gym
from torch.optim.adam import Adam

import log_helper
from SAC_Implementation.ReplayBuffer import ReplayBuffer
from SAC_Implementation.networks import ValueNetwork, SoftQNetwork, PolicyNetwork


def show_replay():
    """
    Not-so-elegant way to display the MP4 file generated by the Monitor wrapper inside a notebook.
    The Monitor wrapper dumps the replay to a local file that we then display as a HTML video object.
    """
    import io
    import base64
    from IPython.display import HTML
    video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()
    encoded = base64.b64encode(video)
    return HTML(data='''
        <video width="360" height="auto" alt="test" controls><source src="data:video/mp4;base64,{0}" type="video/mp4" /></video>'''
                .format(encoded.decode('ascii')))


def get_variable(x):
    """ Converts tensors to cuda, if available. """
    if torch.cuda.is_available():
        return x.cuda()
    return x


def get_numpy(x):
    """ Get numpy array for both cuda and not. """
    if torch.cuda.is_available():
        return x.cpu().data.numpy()
    return x.data.numpy()


def one_hot(l):
    def one_hot2(i):
        """
        One-hot encoder for the states
        """
        a = np.zeros((len(i), l))
        a[range(len(i)), i] = 1
        return a
    return one_hot2

##
def run_sac(hyperparameter_space: dict) -> None:
    """
    Method to to start the SAC algorithm on a certain problem
    :param hyperparameter_space: Dict with the hyperparameter from the Argument parser
    :return:
    """
    log_helper.print_big_log('Initialize Hyperparameter')

    ##
    environment_name = hyperparameter_space.get('env_name')
    # TODO Update
    env = dmc2gym.make(domain_name="hopper", task_name="stand", seed=1, frame_skip=1)

    s = env.reset()
    a = env.action_space.sample()
    logging.debug(f'sample state: {s}')
    logging.debug(f'sample action:{a}')
    ## frame skip = 4
    ## Card pole = 8
    ## Finka task = 2
    # Hyperparameters
    action_dim = env.action_space.shape[0]
    state_dim = env.observation_space.shape[0]

    ##
    logging.debug(f'state shape: {state_dim}')
    logging.debug(f'action shape: {action_dim}')

    episodes = hyperparameter_space.get('episodes')
    sample_batch_size = hyperparameter_space.get('sample_batch_size')

    gamma = hyperparameter_space.get('gamma')

    update_episodes = hyperparameter_space.get('update_episodes')
    hidden_dim = hyperparameter_space.get('hidden_dim')
    lr_critic = hyperparameter_space.get('lr_critic')  # you know this by now
    lr_actor = hyperparameter_space.get('lr_actor')  # you know this by now
    lr_policy = hyperparameter_space.get('lr_policy')  # you know this by now
    discount_factor = hyperparameter_space.get('discount_factor')  # reward discount factor (gamma), 1.0 = no discount
    replay_buffer_size = hyperparameter_space.get('replay_buffer_size')
    n_hidden_layer = hyperparameter_space.get('n_hidden_layer')
    n_hidden = hyperparameter_space.get('n_hidden')
    target_smoothing = hyperparameter_space.get('target_smoothing')
    val_freq = hyperparameter_space.get('val_freq')  # validation frequency
    episodes = hyperparameter_space.get('episodes')
    alpha = hyperparameter_space.get('alpha')
    tau = hyperparameter_space.get('tau')
    # optimizer = optim.Adam(nn.parameters(), lr=learning_rate)

    # Print the hyperparameters
    log_helper.print_dict(hyperparameter_space, "Hyperparameter")
    log_helper.print_big_log("Start Training")

    # Initialization of the Networks
    # ### Actor The actor tries to mimic the Environment and tries to find the expected reward using the next state
    # and the action (from the policy network)

    # We need to networks: 1 for the value function first
    soft_q1 = SoftQNetwork(state_dim, action_dim, hidden_dim, lr_critic)
    soft_q2 = SoftQNetwork(state_dim, action_dim, hidden_dim, lr_critic)

    # Then another one for calculating the targets
    soft_q1_targets = SoftQNetwork(state_dim, action_dim, hidden_dim, lr_critic)
    soft_q2_targets = SoftQNetwork(state_dim, action_dim, hidden_dim, lr_critic)

    policy = PolicyNetwork(state_dim, action_dim, hidden_dim, lr_policy)

    buffer = ReplayBuffer(state_dim, action_dim,
                          replay_buffer_size)

    for _episode in range(episodes):
        logging.debug(f"Episode {_episode+1}")

        # Observe state and action
        current_state = env.reset()
        # The policy network returns the mean and the std of the action. However, we only need an action to start
        action_mean, _ = policy(torch.Tensor(current_state))

        # Do the next step
        logging.debug(f"Our action we chose is : {action_mean}")
        s1, r, done, _ = env.step(np.array(action_mean.detach()))
        buffer.add(obs=current_state, action=action_mean.detach(), reward=r, next_obs=s1, done=done)

        if bool(done):
            break

        for _up_epi in range(update_episodes):
            logging.debug(f"Episode {_episode+1} | {_up_epi+1}")

            soft_q1.optimizer.zero_grad()
            soft_q2.optimizer.zero_grad()

            # Sample from Replay buffer
            state, action, reward, new_state, done, _ = buffer.sample(batch_size=sample_batch_size)

            # Computation of targets
            # Here we are using 2 different Q Networks and afterwards chose the lower reward as regulator.
            y_hat_q1 = soft_q1_targets(state.float(), action.float())
            y_hat_q2 = soft_q2_targets(state.float(), action.float())
            y_hat_q = torch.min(y_hat_q1, y_hat_q2)

            # Sample the action for the new state using the policy network
            action, action_entropy = policy(torch.Tensor(new_state))

            # We calculate the estimated reward for the next state
            # TODO Check the average (We take the mean of the entropy right now)
            y_hat = reward + gamma*(1-done) * (y_hat_q - torch.mean(action_entropy))

            ## Forward step of the Actor network
            # Q1 Network
            q1_forward = soft_q1(state.float(), action.float())
            q1_loss = F.mse_loss(q1_forward.float(), y_hat.float())
            q1_loss.backward(retain_graph=True)
            soft_q1.optimizer.step()

            # Q2 Network
            q2_forward = soft_q2(state.float(), action.float())
            q2_loss = F.mse_loss(q2_forward.float(), y_hat.float().float())
            q2_loss.backward()
            # q2_loss.backward()
            soft_q2.optimizer.step()

            # Update Policy Network
            action_new, action_entropy_new = policy(torch.Tensor(state))
            q1_forward = soft_q1(state.float(), action_new.float())
            q2_forward = soft_q2(state.float(), action_new.float())
            q_forward = torch.min(q1_forward, q2_forward)

            # TODO AGAIN: Check the averaging
            policy_loss = q_forward - (alpha*torch.mean(action_entropy_new))
            policy_loss.backward(torch.Tensor(sample_batch_size, 1))
            policy.optimizer.step()

            # Copy the values for the target network over
            # for param, target_param in zip(soft_q1.parameters(), soft_q1_targets.parameters()):
            #     target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)
            #
            # for param, target_param in zip(soft_q2.parameters(), soft_q2_targets.parameters()):
            #     target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)
            soft_q1_targets.update_params(soft_q1.state_dict(), tau)
            soft_q2_targets.update_params(soft_q2.state_dict(), tau)

            #     logging.debug(param)
            #     logging.debug(target_param)

        # Execute a in the environment
        # Check if it is terminal -> Save in Replay Buffer
        # ---> Reset if

    """import gym
    env = gym.make("Taxi-v3")
    observation = env.reset()
    for _ in range(1000):
      env.render()
      action = env.action_space.sample() # your agent here (this takes random actions)
      observation, reward, done, info = env.step(action)"""
